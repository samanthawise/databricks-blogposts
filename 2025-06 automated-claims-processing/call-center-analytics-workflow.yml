# Databricks Workflow: AI-Powered Call Center Analytics
# Complete end-to-end orchestration of the call center analytics pipeline
#
# Paste the contents of this file into the YAML tab when creating/editing a job in the UI.

resources:
  dashboards:
    call_center_analytics_dashboard:
      display_name: "Call Center Analytics Dashboard"
      file_path: ./dashboards/call-center-analytics.lvdash.json
      warehouse_id: "862f1d757f0424f7"
      parent_path: /Users/${workspace.current_user.userName}/dashboards

  jobs:
    call_center_analytics_pipeline:
      name: call-center-analytics-pipeline

      queue:
        enabled: true

      # Job-level configuration
      job_clusters:
        - job_cluster_key: main_cluster
          new_cluster:
            spark_version: 14.3.x-scala2.12
            node_type_id: i3.xlarge
            num_workers: 2
            autoscale:
              min_workers: 1
              max_workers: 4
            spark_conf:
              spark.databricks.delta.optimizeWrite.enabled: "true"
              spark.databricks.delta.autoCompact.enabled: "true"
            custom_tags:
              project: call-center-analytics
              environment: production

      # Pipeline tasks
      tasks:
        # Task 1: Setup - Initialize catalog, schema, volumes, lookup tables
        - task_key: setup_infrastructure
          job_cluster_key: main_cluster
          notebook_task:
            notebook_path: /Workspace/Users/${workspace.current_user.userName}/2025-06 automated-claims-processing/setup/00-setup.py
            base_parameters:
              reset_all_data: "false"
          timeout_seconds: 600
          libraries:
            - pypi:
                package: faker>=20.0.0

        # Task 2: Generate Synthetic Call Data
        - task_key: generate_sample_data
          depends_on:
            - task_key: setup_infrastructure
          job_cluster_key: main_cluster
          notebook_task:
            notebook_path: /Workspace/Users/${workspace.current_user.userName}/2025-06 automated-claims-processing/setup/sample_data_generator.py
          timeout_seconds: 600
          libraries:
            - pypi:
                package: faker>=20.0.0

        # Task 3: Run Gold Layer AI Enrichment Pipeline
        - task_key: gold_layer_enrichment
          depends_on:
            - task_key: generate_sample_data
          job_cluster_key: main_cluster
          notebook_task:
            notebook_path: /Workspace/Users/${workspace.current_user.userName}/2025-06 automated-claims-processing/pipeline/02-sdp-bronze-silver-gold.py
          timeout_seconds: 3600
          retry_on_timeout: true
          max_retries: 2
          min_retry_interval_millis: 300000
          libraries:
            - pypi:
                package: mutagen>=1.47.0

        # Task 4: Create Unity Catalog Functions for Agent Integration
        - task_key: create_uc_functions
          depends_on:
            - task_key: gold_layer_enrichment
          job_cluster_key: main_cluster
          notebook_task:
            notebook_path: /Workspace/Users/${workspace.current_user.userName}/2025-06 automated-claims-processing/tools/03-create-uc-tools.py
            base_parameters:
              recreate_uc_tools: "false"
          timeout_seconds: 300

        # Task 5: Prepare Dashboard Aggregation Tables
        - task_key: prepare_dashboard_data
          depends_on:
            - task_key: gold_layer_enrichment
          job_cluster_key: main_cluster
          notebook_task:
            notebook_path: /Workspace/Users/${workspace.current_user.userName}/2025-06 automated-claims-processing/notebooks/05-prepare-dashboard-data.py
          timeout_seconds: 600

      # Schedule (currently paused - set pause_status: UNPAUSED to enable)
      schedule:
        quartz_cron_expression: "0 0 2 * * ?"
        timezone_id: "UTC"
        pause_status: PAUSED

      # Email notifications
      email_notifications:
        on_start:
          - <DATA_ENGINEERING_EMAIL>
        on_success:
          - <DATA_ENGINEERING_EMAIL>
        on_failure:
          - <DATA_ENGINEERING_EMAIL>
          - <ONCALL_EMAIL>

      max_concurrent_runs: 1
      timeout_seconds: 7200

      # Parameters (can be overridden at runtime)
      parameters:
        - name: catalog
          default: call_center_analytics
        - name: schema
          default: main
        - name: volume
          default: call_center_data

      # Tags for organization
      tags:
        project: call-center-analytics
        environment: production
        data_classification: sensitive

      # Access control (DAB format)
      # Uncomment and replace placeholders before deploying to prod:
      # permissions:
      #   - user_name: <DATA_ENGINEERING_TEAM_EMAIL>
      #     level: CAN_MANAGE
      #   - group_name: data-engineers
      #     level: CAN_MANAGE
      #   - group_name: data-scientists
      #     level: CAN_VIEW

# =====================
# DEPLOYMENT INSTRUCTIONS
# =====================
#
# BEFORE PASTING:
# Replace all placeholders with your actual values:
#   - ${workspace.current_user.userName}: Your Databricks workspace username
#   - <DATA_ENGINEERING_EMAIL>: Email for notifications
#   - <ONCALL_EMAIL>: Email for failure alerts
#   - <DATA_ENGINEERING_TEAM_EMAIL>: Email for access control
#
# PRODUCTION CHECKLIST:
# [ ] All <placeholders> replaced
# [ ] node_type_id updated for your cloud provider/region
# [ ] Schedule set to UNPAUSED if automated runs are needed
# [ ] Test run completed with all 5 tasks passing
# [ ] Gold table has phone_numbers/email_addresses as ARRAY types
# [ ] 8 UC Functions created successfully
# [ ] 5 dashboard aggregation tables created
