# Lakeflow Pipeline Configuration
# AI-Powered Call Center Analytics

name: call-center-analytics-pipeline
description: End-to-end pipeline for call center audio transcription and AI-powered analysis

# Cluster Configuration
cluster:
  spark_version: "14.3.x-scala2.12"  # Use latest LTS runtime
  node_type_id: "i3.xlarge"  # Adjust based on workload
  autoscale:
    min_workers: 1
    max_workers: 4
  spark_conf:
    "spark.databricks.delta.optimizeWrite.enabled": "true"
    "spark.databricks.delta.autoCompact.enabled": "true"
  custom_tags:
    project: "call-center-analytics"
    environment: "production"

# Library Dependencies
libraries:
  - pypi:
      package: "mutagen>=1.47.0"
  - pypi:
      package: "faker>=20.0.0"

# Pipeline Stages
stages:
  # Stage 1: Setup and Initialization
  - name: setup
    notebook_path: "./setup/00-setup.py"
    timeout_seconds: 600
    depends_on: []
    parameters:
      reset_all_data: "false"

  # Stage 2: Deploy Whisper Endpoint (if not exists)
  - name: deploy-whisper-endpoint
    notebook_path: "./setup/01-deploy-whisper-endpoint.py"
    timeout_seconds: 1800  # 30 minutes for endpoint deployment
    depends_on:
      - setup

  # Stage 3: Generate Sample Data (optional, for demo)
  - name: generate-sample-data
    notebook_path: "./setup/sample_data_generator.py"
    timeout_seconds: 900
    depends_on:
      - deploy-whisper-endpoint
    enabled: false  # Set to true to generate demo data

  # Stage 4: Run SDP Pipeline (Bronze → Silver → Gold)
  - name: bronze-silver-gold-pipeline
    notebook_path: "./pipeline/02-sdp-bronze-silver-gold.py"
    timeout_seconds: 3600  # 1 hour for full pipeline
    depends_on:
      - deploy-whisper-endpoint
    retry_on_failure: 2
    retry_delay_seconds: 300

  # Stage 5: Create UC Functions
  - name: create-uc-functions
    notebook_path: "./tools/03-create-uc-tools.py"
    timeout_seconds: 300
    depends_on:
      - bronze-silver-gold-pipeline

  # Stage 6: Prepare Dashboard Data
  - name: prepare-dashboard-data
    notebook_path: "./notebooks/05-prepare-dashboard-data.py"
    timeout_seconds: 600
    depends_on:
      - bronze-silver-gold-pipeline

# Triggers
triggers:
  # Manual trigger
  - trigger_type: manual

  # File arrival trigger - runs when new audio files are uploaded
  - trigger_type: file_arrival
    paths:
      - "/Volumes/call_center_analytics/main/call_center_data/raw_recordings"
    file_pattern: "*.wav,*.mp3,*.flac"

  # Schedule trigger - runs daily at 2 AM
  - trigger_type: scheduled
    cron_expression: "0 2 * * *"
    timezone: "UTC"
    pause_status: paused  # Set to 'active' to enable scheduled runs

# Notifications
notifications:
  on_success:
    - email:
        - "data-engineering@example.com"
  on_failure:
    - email:
        - "data-engineering@example.com"
        - "oncall@example.com"
  on_timeout:
    - email:
        - "data-engineering@example.com"

# Pipeline Settings
settings:
  max_concurrent_runs: 1  # Prevent concurrent pipeline runs
  timeout_seconds: 7200  # 2 hours for entire pipeline
  continuous: false  # Batch processing mode

# Unity Catalog Configuration
catalog_settings:
  catalog: "call_center_analytics"
  schema: "main"
  volume: "call_center_data"

# Checkpoints and Storage
storage:
  checkpoints_location: "/Volumes/call_center_analytics/main/call_center_data/_checkpoints"
  temp_location: "/tmp/call-center-analytics"

# Performance Optimization
optimization:
  enable_photon: true
  enable_adaptive_query_execution: true
  enable_delta_optimizations: true

# Monitoring and Observability
monitoring:
  enable_metrics: true
  enable_logging: true
  log_level: "INFO"

# Environment-Specific Overrides
environments:
  dev:
    cluster:
      node_type_id: "i3.xlarge"
      autoscale:
        min_workers: 1
        max_workers: 2
    settings:
      timeout_seconds: 3600

  staging:
    cluster:
      node_type_id: "i3.xlarge"
      autoscale:
        min_workers: 1
        max_workers: 3

  prod:
    cluster:
      node_type_id: "i3.2xlarge"
      autoscale:
        min_workers: 2
        max_workers: 8
    notifications:
      on_failure:
        - pagerduty:
            integration_key: "${pagerduty_key}"
