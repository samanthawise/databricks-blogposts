{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "067e02d6-78ef-418b-822c-be2293588dd5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Create & Populate Raw Transcriptions Table"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS claims_transcriptions (\n",
    "#     file_id STRING GENERATED ALWAYS AS (sha2(concat(file_path, modificationTime), 256)),\n",
    "#     file_path STRING,\n",
    "#     modificationTime TIMESTAMP,\n",
    "#     name STRING,\n",
    "#     size BIGINT,\n",
    "#     transcription STRING\n",
    "# ) USING DELTA;\n",
    "\n",
    "# INSERT INTO claims_transcriptions\n",
    "# SELECT sha2(concat(file_path, modificationTime), 256), file_path, modificationTime, name, size, transcription\n",
    "# FROM source_table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "862f152a-be08-45f6-b7aa-5fdb4d542053",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Extract AI Features & Store in Processed Data Table sql Copy Edit"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create the table for claims analysis if it doesn't exist\n",
    "CREATE TABLE IF NOT EXISTS samantha_wise.ai_claims_processing.claims_analysis (\n",
    "    sentiment STRING,\n",
    "    topic STRING,\n",
    "    full_name STRING,\n",
    "    date_of_birth STRING,\n",
    "    post_code STRING,\n",
    "    processing_time TIMESTAMP\n",
    ") USING DELTA;\n",
    "\n",
    "-- Insert results from transcription analysis\n",
    "INSERT INTO samantha_wise.ai_claims_processing.claims_analysis\n",
    "SELECT\n",
    "    ai_analyze_sentiment(transcription) AS sentiment,\n",
    "    ai_extract_topics(transcription) AS topic,\n",
    "    ai_extract(transcription, 'PERSON') AS full_name,\n",
    "    ai_extract(transcription, 'DATE') AS date_of_birth,\n",
    "    ai_extract(transcription, 'GPE') AS post_code,\n",
    "    current_timestamp() AS processing_time\n",
    "FROM samantha_wise.ai_claims_processing.transcriptions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04596c63-f349-4a34-9761-6ffec5a12578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    get_json_object(topic, '$.topic') AS topic_name, \n",
    "    COUNT(*) AS topic_count\n",
    "FROM samantha_wise.ai_claims_processing.transcriptions\n",
    "GROUP BY topic_name\n",
    "ORDER BY topic_count DESC;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1315887242949104,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "ETL (AD - to archive)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
