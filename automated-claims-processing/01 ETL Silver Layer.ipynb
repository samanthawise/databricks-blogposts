{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "069fb7cf-8c99-4b7c-9df1-d9e262c98fb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ”„ Notebook: 01 ETL Silver Layer\n",
    "\n",
    "This notebook handles the **Silver Layer (Data Transformation & Processing)** in the Medallion Architecture of the AI-powered claims pipeline. It focuses on **audio conversion**, **metadata extraction**, and **speech-to-text transcription** using the [OpenAI Whisper model](https://openai.com/index/whisper/).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§± Purpose\n",
    "\n",
    "To convert raw audio files into a consistent format (MP3), calculate metadata (duration), and transcribe the content into structured text to support downstream AI analytics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c6de7b-4322-4d7e-a1aa-777981e9ee68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Dependencies and Restart Python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pydub mutagen openai-whisper numpy>=1.24\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21fbef5f-8d95-4719-9119-e7d696ef61eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Initial Resources for Notebook Execution"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./resources/init\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65dfc242-a7ad-4b30-aa69-03a646bc360d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert Files to MP3 and Save to New Volume"
    }
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "file_reference_df = spark.table(f\"{CATALOG}.{SCHEMA}.recordings_file_reference_bronze\")\n",
    "\n",
    "mp3_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/mp3_audio_recordings/\"\n",
    "if not dbutils.fs.mkdirs(mp3_path):\n",
    "    dbutils.fs.mkdirs(mp3_path)\n",
    "\n",
    "# Convert each file to mp3 and save to the new volume\n",
    "for row in file_reference_df.collect():\n",
    "    file_path = row['file_path']\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    new_file_path = os.path.join(mp3_path, os.path.basename(file_path).replace(os.path.splitext(file_path)[1], \".mp3\"))\n",
    "    audio.export(new_file_path, format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd64d4c3-be1c-4ede-b9a4-096c573aad3b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create DataFrame with MP3 Audio Durations"
    }
   },
   "outputs": [],
   "source": [
    "from mutagen.mp3 import MP3\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "mp3_file_reference_df = spark.createDataFrame(\n",
    "    dbutils.fs.ls(f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/mp3_audio_recordings\")\n",
    ").withColumn(\"file_path\", F.expr(\"substring(path, 6, length(path))\"))\n",
    "\n",
    "def get_audio_duration(file_path):\n",
    "    audio = MP3(file_path)\n",
    "    return audio.info.length\n",
    "\n",
    "get_audio_duration_udf = F.udf(get_audio_duration, FloatType())\n",
    "\n",
    "mp3_file_reference_df = mp3_file_reference_df.withColumn(\"audio_duration\", F.round(get_audio_duration_udf(\"file_path\"), 0))\n",
    "\n",
    "display(mp3_file_reference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5416e38-cce5-40a5-b4ca-36b6c48ea8a5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Whisper Model and Print Status"
    }
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# Load Whisper model (choose \"small\" for CPU, \"medium\" or \"large\" for GPU)\n",
    "model = whisper.load_model(\"small\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f174cbfe-f13c-475f-abfb-5a70f00a4e4b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transcribe Audio with Whisper Model"
    }
   },
   "outputs": [],
   "source": [
    "def transcribe_audio(file_path: str, model: whisper.Whisper) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe audio using Whisper model.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the audio file.\n",
    "        model (whisper.Whisper): Whisper model instance.\n",
    "\n",
    "    Returns:\n",
    "        str: Transcribed text from the audio file.\n",
    "    \"\"\"\n",
    "    result = model.transcribe(file_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f66114-0ee3-453e-9678-6ed6be1165ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transcribe Audio Files to Text with Spark_UDF"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, regexp_replace, to_timestamp, concat_ws\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Collect the file paths to the driver\n",
    "file_paths = mp3_file_reference_df.select(\"file_path\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Transcribe the audio files outside of Spark\n",
    "transcriptions = [transcribe_audio(file_path, model) for file_path in file_paths]\n",
    "\n",
    "# Create a DataFrame with the transcriptions\n",
    "transcriptions_df = spark.createDataFrame(zip(file_paths, transcriptions), [\"file_path\", \"transcription\"])\n",
    "\n",
    "# Join the transcriptions back to the original DataFrame\n",
    "result_df = mp3_file_reference_df.join(transcriptions_df, on=\"file_path\", how=\"inner\") \\\n",
    "                                 .select(\"path\", \"modificationTime\", \"file_path\", \"transcription\", \"audio_duration\")\n",
    "\n",
    "result_df = result_df.withColumn(\"file_name\", split(col(\"file_path\"), \"/\").getItem(6)) \\\n",
    "    .withColumn(\"file_name\", regexp_replace(col(\"file_name\"), \".mp3\", \"\")) \\\n",
    "    .withColumn(\"call_id\", split(col(\"file_name\"), \"_\").getItem(0)) \\\n",
    "    .withColumn(\"agent_id\", split(col(\"file_name\"), \"_\").getItem(1)) \\\n",
    "    .withColumn(\"call_datetime\", \n",
    "        to_timestamp(\n",
    "            concat_ws(\":\", split(col(\"file_name\"), \"_\").getItem(2), \n",
    "            split(col(\"file_name\"), \"_\").getItem(3), \n",
    "            split(col(\"file_name\"), \"_\").getItem(4)))\n",
    "    )\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75e6e4a-51e4-4d3a-a9c6-f86754f92781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# existing_transcriptions_df = spark.table(f\"{CATALOG}.{SCHEMA}.simulated_transcriptions\")\n",
    "# combined_transcriptions_df = existing_transcriptions_df.unionByName(result_df)\n",
    "\n",
    "# combined_transcriptions_df = combined_transcriptions_df.withColumn(\"file_name\", split(col(\"file_path\"), \"/\").getItem(6)) \\\n",
    "#     .withColumn(\"file_name\", regexp_replace(col(\"file_name\"), \".mp3\", \"\")) \\\n",
    "#     .withColumn(\"call_id\", split(col(\"file_name\"), \"_\").getItem(0)) \\\n",
    "#     .withColumn(\"agent_id\", split(col(\"file_name\"), \"_\").getItem(1)) \\\n",
    "#     .withColumn(\"call_datetime\", \n",
    "#         to_timestamp(\n",
    "#             concat_ws(\":\", split(col(\"file_name\"), \"_\").getItem(2), \n",
    "#             split(col(\"file_name\"), \"_\").getItem(3), \n",
    "#             split(col(\"file_name\"), \"_\").getItem(4))))\n",
    "    \n",
    "\n",
    "# display(combined_transcriptions_df)\n",
    "\n",
    "# combined_transcriptions_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.transcriptions_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be9a448c-06ee-411a-b346-d8aa01785479",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Combine and Save Transcriptions Data in Silver Table"
    }
   },
   "outputs": [],
   "source": [
    "if spark.catalog.tableExists(f\"{CATALOG}.{SCHEMA}.transcriptions_silver\"):\n",
    "    transcriptions_silver_df = spark.table(f\"{CATALOG}.{SCHEMA}.transcriptions_silver\")\n",
    "    combined_transcriptions_df = transcriptions_silver_df.unionByName(result_df).dropDuplicates()\n",
    "else:\n",
    "    combined_transcriptions_df = result_df.dropDuplicates()\n",
    "\n",
    "combined_transcriptions_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.transcriptions_silver\")\n",
    "\n",
    "display(combined_transcriptions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2e353f4-c0b0-4782-95a9-5b52340c27a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… Output\n",
    "- A clean, enriched Delta table: transcriptions_silver\n",
    "- Includes transcription text, call metadata, and audio duration for each entry."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01 ETL Silver Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
