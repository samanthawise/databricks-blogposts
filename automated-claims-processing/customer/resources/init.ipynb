{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb79b31-dd96-433c-9cd3-b108ba723759",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration Parameters"
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = 'samantha_wise'\n",
    "SCHEMA = 'ai_claims_processing_customer'\n",
    "VOLUME = 'audio_recordings'\n",
    "\n",
    "BRONZE_TABLE = 'recordings_file_reference_bronze'\n",
    "SILVER_TABLE = 'transcriptions_silver'\n",
    "GOLD_TABLE = 'analysis_gold'\n",
    "\n",
    "META_TABLE = 'meta_data'\n",
    "CALL_CENTER_REASONS_TABLE = 'call_centre_reasons'\n",
    "\n",
    "RAW_DIR = 'raw_recordings'\n",
    "MP3_DIR = 'mp3_audio_recordings'\n",
    "\n",
    "# Path for raw audio files\n",
    "raw_audio_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{RAW_DIR}/\"\n",
    "mp3_audio_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{MP3_DIR}/\"\n",
    "\n",
    "# Optional: Default LLM endpoint (used later in pipeline stages)\n",
    "ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feef4626-65b5-4a59-bf28-599553a78292",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ensure Catalog, Schema, and Volume exist"
    }
   },
   "outputs": [],
   "source": [
    "if not spark.sql(f\"SHOW CATALOGS LIKE '{CATALOG}'\").count():\n",
    "    spark.sql(f\"CREATE CATALOG `{CATALOG}`\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{CATALOG}`.`{SCHEMA}`\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{CATALOG}`.`{SCHEMA}`.`{VOLUME}`\")\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{CATALOG}`.`{SCHEMA}`.`checkpoints`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4030522e-cf1b-4a9c-9363-4cf90016de98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(f\"/Volumes/{CATALOG}/{SCHEMA}/checkpoints/raw_audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51f0cb7c-1502-4894-ba4b-ec62b14297f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Map Call Reasons to Next Steps and Save as Table"
    }
   },
   "outputs": [],
   "source": [
    "call_center_reasons_table_name = f\"{CATALOG}.{SCHEMA}.{CALL_CENTER_REASONS_TABLE}\"\n",
    "\n",
    "if not spark._jsparkSession.catalog().tableExists(call_center_reasons_table_name):\n",
    "  from pyspark.sql.functions import when\n",
    "\n",
    "  reasons_dict = {\n",
    "      \"Claim status inquiry\": \"Provide claim status update\",\n",
    "      \"Coverage details request\": \"Explain coverage details\",\n",
    "      \"Billing and premium question\": \"Assist with billing\",\n",
    "      \"Finding in-network provider\": \"Find in-network provider\",\n",
    "      \"Policy renewal\": \"Initiate policy renewal\",\n",
    "      \"Updating personal details\": \"Update customer details\",\n",
    "      \"Technical support\": \"Provide technical support\",\n",
    "      \"Filing a new claim\": \"File new claim request\",\n",
    "      \"Canceling a policy\": \"Process policy cancellation\"\n",
    "  }\n",
    "\n",
    "  financial_hardship_dict = {\n",
    "      \"Requesting premium payment deferral due to financial hardship\": \"Review eligibility for payment deferral\",\n",
    "      \"Inquiry about hardship assistance programs\": \"Explain available financial hardship assistance options\",\n",
    "      \"Request to lower coverage temporarily due to income loss\": \"Adjust policy coverage as requested\"\n",
    "  }\n",
    "\n",
    "  fraud_dict = {\n",
    "      \"Fraudulent claim attempt\": \"Escalate suspected fraud\"\n",
    "  }\n",
    "\n",
    "  # Combine all reasons for general selection\n",
    "  all_reason_mappings = list(reasons_dict.items())\n",
    "\n",
    "  # Combine all_reason_mappings, financial_hardship_dict, and fraud_dict\n",
    "  combined_reason_mappings = all_reason_mappings + list(financial_hardship_dict.items()) + list(fraud_dict.items())\n",
    "\n",
    "  # Convert combined_reason_mappings into a DataFrame\n",
    "  df_reasons = spark.createDataFrame(combined_reason_mappings, [\"reason_for_call\", \"next_steps\"])\n",
    "\n",
    "  # Update the rows where reason_for_call comes from financial_hardship_dict to 'Financial hardship'\n",
    "  df_reasons = df_reasons.withColumn(\n",
    "      \"reason_for_call\",\n",
    "      when(df_reasons[\"reason_for_call\"].isin(list(financial_hardship_dict.keys())), \"Financial hardship\").otherwise(df_reasons[\"reason_for_call\"])\n",
    "  )\n",
    "\n",
    "  # Save the DataFrame as a table\n",
    "  df_reasons.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.call_centre_reasons\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "init",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
